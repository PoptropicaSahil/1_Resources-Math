{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to A/B Testing\n",
    "\n",
    "A/B testing, also known as split testing, is a method used to compare two versions of a product, webpage, or algorithm to determine which one performs better. In machine learning and data analysis, A/B testing is often used to evaluate the performance of different models, features, or strategies.\n",
    "The basic idea is to randomly divide your user base or dataset into two groups:\n",
    "\n",
    "- Group A: The control group, which receives the current version (A)\n",
    "- Group B: The treatment group, which receives the new version (B)\n",
    "\n",
    "You then collect data on how each group performs according to your chosen metric(s) and use statistical analysis to determine if there's a significant difference between the two groups.\n",
    "\n",
    "### Sample Dataset\n",
    "\n",
    "Let's create a sample dataset for an e-commerce website that wants to test two different recommendation algorithms:\n",
    "\n",
    "- Algorithm A: The current recommendation system\n",
    "- Algorithm B: A new recommendation system\n",
    "\n",
    "Our metric will be the click-through rate (CTR) of recommended products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id group  impressions  clicks       ctr\n",
      "0        1     A           72       3  0.041667\n",
      "1        2     B           26      16  0.615385\n",
      "2        3     A           82       2  0.024390\n",
      "3        4     A           42      12  0.285714\n",
      "4        5     A           93       8  0.086022\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate sample data\n",
    "n_users = 1000\n",
    "data = {\n",
    "    \"user_id\": range(1, n_users + 1),\n",
    "    \"group\": np.random.choice([\"A\", \"B\"], size=n_users),\n",
    "    \"impressions\": np.random.randint(10, 100, size=n_users),\n",
    "    \"clicks\": np.random.randint(0, 20, size=n_users),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df[\"ctr\"] = df[\"clicks\"] / df[\"impressions\"]\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A-B Testing\n",
    "Separates the data into two groups, calculates the mean CTR for each group, and performs a two-sample t-test to compare the means.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CTR for Group A: 0.2683\n",
      "Mean CTR for Group B: 0.2468\n",
      "T-statistic: 1.2296\n",
      "P-value: 0.2191\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Separate the groups\n",
    "group_a = df[df['group'] == 'A']\n",
    "group_b = df[df['group'] == 'B']\n",
    "\n",
    "# Calculate mean CTR for each group\n",
    "ctr_a = group_a['ctr'].mean()\n",
    "ctr_b = group_b['ctr'].mean()\n",
    "\n",
    "print(f\"Mean CTR for Group A: {ctr_a:.4f}\")\n",
    "print(f\"Mean CTR for Group B: {ctr_b:.4f}\")\n",
    "\n",
    "# Perform t-test\n",
    "t_statistic, p_value = stats.ttest_ind(group_a['ctr'], group_b['ctr'])\n",
    "\n",
    "print(f\"T-statistic: {t_statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Analysis\n",
    "\n",
    "The t-test helps us determine if there's a statistically significant difference between the two groups. Here's how to interpret the results:\n",
    "\n",
    "- T-statistic: Measures the difference between the two group means relative to the variation in the data. A larger absolute value indicates a greater difference between the groups.\n",
    "- P-value: The probability of obtaining test results at least as extreme as the observed results, assuming that the null hypothesis is true. In A/B testing, the null hypothesis is typically that there's no difference between the groups.\n",
    "\n",
    "    - If p-value < significance level (usually 0.05), we reject the null hypothesis and conclude that there's a significant difference between the groups.\n",
    "    - If p-value â‰¥ significance level, we fail to reject the null hypothesis and cannot conclude that there's a significant difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interpretation:\n",
      "There is no statistically significant difference between the two groups.\n",
      "\n",
      "Relative improvement: -7.98%\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05  # Significance level\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "if p_value < alpha:\n",
    "    print(\"There is a statistically significant difference between the two groups.\")\n",
    "    if ctr_b > ctr_a:\n",
    "        print(\n",
    "            \"Group B (new algorithm) performs better than Group A (current algorithm).\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            \"Group A (current algorithm) performs better than Group B (new algorithm).\"\n",
    "        )\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between the two groups.\")\n",
    "\n",
    "# Calculate relative improvement\n",
    "relative_improvement = (ctr_b - ctr_a) / ctr_a * 100\n",
    "print(f\"\\nRelative improvement: {relative_improvement:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confidence Intervals (95%):\n",
      "Group A: 0.2683 (0.2434 - 0.2931)\n",
      "Group B: 0.2468 (0.2233 - 0.2704)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Calculate confidence interval\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    m = np.mean(data)\n",
    "    std_err = stats.sem(data)\n",
    "    h = std_err * t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return m, m - h, m + h\n",
    "\n",
    "\n",
    "ci_a = confidence_interval(group_a[\"ctr\"])\n",
    "ci_b = confidence_interval(group_b[\"ctr\"])\n",
    "\n",
    "print(\"\\nConfidence Intervals (95%):\")\n",
    "print(f\"Group A: {ci_a[0]:.4f} ({ci_a[1]:.4f} - {ci_a[2]:.4f})\")\n",
    "print(f\"Group B: {ci_b[0]:.4f} ({ci_b[1]:.4f} - {ci_b[2]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
